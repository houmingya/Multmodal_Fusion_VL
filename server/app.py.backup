# ====================================
# 服务器端代码 - FastAPI服务
# ====================================
# 功能：提供VQA图文问答和文搜图接口
# 环境：12G显存工作站 + Python 3.10+
# 依赖：modelscope, torch, fastapi, uvicorn, faiss-cpu, Pillow

import os
import io
import base64
import torch
import numpy as np
from typing import Optional
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
from PIL import Image
import logging

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ====================================
# 配置项（可根据实际情况修改）
# ====================================
SERVER_HOST = "0.0.0.0"  # 服务器监听地址
SERVER_PORT = 8000       # 服务器端口
IMAGE_LIBRARY_PATH = "./image_library"  # 服务器端图片库路径
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # 自动检测GPU

# 创建图片库目录（如不存在）
os.makedirs(IMAGE_LIBRARY_PATH, exist_ok=True)

# ====================================
# 全局模型加载（仅启动时加载一次）
# ====================================
app = FastAPI(title="多模态融合服务", description="VQA图文问答 + 文搜图服务")

# 全局变量存储模型
vqa_model = None
vqa_processor = None
clip_model = None
clip_preprocessor = None
clip_tokenizer = None
image_library = {}  # 存储图片库：{文件名: 特征向量}

def load_vqa_model():
    """
    加载LLaVA图文问答模型（12G显存优化版）
    优化策略：FP16精度 + 自动设备分配
    """
    global vqa_model, vqa_processor
    try:
        logger.info("正在加载LLaVA-7B模型（FP16优化版）...")
        from modelscope import AutoModelForCausalLM, AutoTokenizer
        
        model_id = "FreedomIntelligence/llava-v1.5-7b-TRIM"
        
        # 使用FP16精度减少显存占用（约7GB显存）
        vqa_model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,  # FP16精度（关键优化）
            device_map="auto",          # 自动分配设备
            trust_remote_code=True,
            low_cpu_mem_usage=True      # 减少CPU内存占用
        )
        
        vqa_processor = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True
        )
        
        # 打印显存占用（验证是否符合12G约束）
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
            logger.info(f"✓ LLaVA模型加载成功！显存占用: {memory_allocated:.2f}GB (预留: {memory_reserved:.2f}GB)")
        else:
            logger.warning("⚠ 未检测到GPU，将使用CPU运行（速度较慢）")
            
    except Exception as e:
        logger.error(f"✗ LLaVA模型加载失败: {str(e)}")
        raise

def load_clip_model():
    """
    加载CLIP图文检索模型（中文轻量版）
    模型：vit-base-patch16（约1GB显存）
    """
    global clip_model, clip_preprocessor, clip_tokenizer
    try:
        logger.info("正在加载CLIP中文轻量模型...")
        from modelscope import Model, AutoTokenizer
        from modelscope.pipelines import pipeline
        from modelscope.utils.constant import Tasks
        
        model_id = "iic/multi-modal_clip-vit-base-patch16_zh"
        
        # 加载CLIP模型（轻量版，约1GB显存）
        clip_model = Model.from_pretrained(model_id)
        clip_model.to(DEVICE)
        clip_model.eval()  # 推理模式
        
        # 加载分词器（处理中文文本）
        clip_tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        # 图像预处理器
        from torchvision import transforms
        clip_preprocessor = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])
        
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
            logger.info(f"✓ CLIP模型加载成功！当前总显存占用: {memory_allocated:.2f}GB")
        
    except Exception as e:
        logger.error(f"✗ CLIP模型加载失败: {str(e)}")
        raise

def build_image_library():
    """
    构建图片库特征索引
    扫描IMAGE_LIBRARY_PATH目录，提取所有图片的CLIP特征向量
    """
    global image_library
    try:
        logger.info(f"正在构建图片库索引（路径: {IMAGE_LIBRARY_PATH}）...")
        
        # 支持的图片格式
        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
        image_files = [
            f for f in os.listdir(IMAGE_LIBRARY_PATH) 
            if os.path.splitext(f.lower())[1] in valid_extensions
        ]
        
        if not image_files:
            logger.warning(f"⚠ 图片库为空！请在 {IMAGE_LIBRARY_PATH} 目录下放置测试图片")
            # 创建示例说明文件
            with open(os.path.join(IMAGE_LIBRARY_PATH, "README.txt"), "w", encoding="utf-8") as f:
                f.write("请将待检索的图片放入此目录\n")
                f.write("支持格式: jpg, jpeg, png, bmp, webp\n")
                f.write("示例文件名: cat.jpg, dog.jpg, sunset.jpg\n")
            return
        
        for img_file in image_files:
            img_path = os.path.join(IMAGE_LIBRARY_PATH, img_file)
            try:
                # 加载图片
                image = Image.open(img_path).convert("RGB")
                
                # 提取CLIP图像特征
                image_tensor = clip_preprocessor(image).unsqueeze(0).to(DEVICE)
                with torch.no_grad():
                    image_features = clip_model.get_image_features(pixel_values=image_tensor)
                    # L2归一化（用于余弦相似度计算）
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # 存储特征向量
                image_library[img_file] = image_features.cpu().numpy()
                logger.info(f"  ✓ 已索引: {img_file}")
                
            except Exception as e:
                logger.warning(f"  ✗ 跳过无效图片 {img_file}: {str(e)}")
        
        logger.info(f"✓ 图片库构建完成！共索引 {len(image_library)} 张图片")
        
    except Exception as e:
        logger.error(f"✗ 图片库构建失败: {str(e)}")

@app.on_event("startup")
async def startup_event():
    """FastAPI启动时自动加载模型"""
    logger.info("=" * 50)
    logger.info("服务器启动中...")
    logger.info("=" * 50)
    
    # 加载VQA模型
    load_vqa_model()
    
    # 加载CLIP模型
    load_clip_model()
    
    # 构建图片库索引
    build_image_library()
    
    logger.info("=" * 50)
    logger.info("✓ 所有模型加载完成！服务已就绪")
    logger.info("=" * 50)

# ====================================
# API接口：图文问答（VQA）
# ====================================
@app.post("/vqa")
async def visual_question_answering(
    image: UploadFile = File(..., description="上传的图片文件"),
    question: str = Form(..., description="中文问题")
):
    """
    图文问答接口
    输入：图片 + 中文问题
    输出：模型回答
    """
    try:
        # 1. 读取图片数据
        image_bytes = await image.read()
        pil_image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        
        logger.info(f"收到VQA请求 - 问题: {question}")
        
        # 2. 构造LLaVA输入格式（简化版推理）
        # 注意：这里使用简化的推理方式，实际需要根据LLaVA-1.5的具体API调整
        from modelscope import pipeline
        
        # 使用modelscope的pipeline（自动处理图像和文本）
        vqa_pipeline = pipeline(
            task='visual-question-answering',
            model=vqa_model,
            preprocessor=vqa_processor
        )
        
        # 3. 模型推理（FP16精度）
        with torch.no_grad():
            result = vqa_pipeline({
                'image': pil_image,
                'question': question
            })
        
        # 4. 提取答案
        answer = result.get('text', '抱歉，无法生成回答')
        
        logger.info(f"VQA回答: {answer}")
        
        return JSONResponse({
            "status": "success",
            "question": question,
            "answer": answer
        })
        
    except Exception as e:
        logger.error(f"VQA推理失败: {str(e)}")
        raise HTTPException(status_code=500, detail=f"推理失败: {str(e)}")

# ====================================
# API接口：文搜图（Text-to-Image Search）
# ====================================
@app.post("/text2image_search")
async def text_to_image_search(
    text_query: str = Form(..., description="中文检索文本"),
    top_k: int = Form(3, description="返回最相似的前K张图片")
):
    """
    文搜图接口
    输入：中文文本描述
    输出：最相似的图片路径列表
    """
    try:
        if not image_library:
            raise HTTPException(status_code=404, detail="图片库为空，请先添加图片到服务器")
        
        logger.info(f"收到文搜图请求 - 查询: {text_query}")
        
        # 1. 提取文本特征
        text_inputs = clip_tokenizer(
            text_query,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=77
        ).to(DEVICE)
        
        with torch.no_grad():
            text_features = clip_model.get_text_features(**text_inputs)
            # L2归一化
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        text_features_np = text_features.cpu().numpy()
        
        # 2. 计算与图片库的相似度（余弦相似度）
        similarities = {}
        for img_name, img_features in image_library.items():
            # 余弦相似度 = 点积（已归一化）
            similarity = np.dot(text_features_np[0], img_features[0])
            similarities[img_name] = float(similarity)
        
        # 3. 排序并返回Top-K
        sorted_results = sorted(
            similarities.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        # 4. 构造返回结果（包含图片Base64编码，便于本地展示）
        results = []
        for img_name, score in sorted_results:
            img_path = os.path.join(IMAGE_LIBRARY_PATH, img_name)
            
            # 读取图片并转为Base64
            with open(img_path, "rb") as f:
                img_base64 = base64.b64encode(f.read()).decode('utf-8')
            
            results.append({
                "image_name": img_name,
                "similarity_score": round(score, 4),
                "image_base64": img_base64
            })
        
        logger.info(f"文搜图完成 - 返回 {len(results)} 张图片")
        
        return JSONResponse({
            "status": "success",
            "query": text_query,
            "results": results
        })
        
    except Exception as e:
        logger.error(f"文搜图失败: {str(e)}")
        raise HTTPException(status_code=500, detail=f"检索失败: {str(e)}")

# ====================================
# 健康检查接口
# ====================================
@app.get("/health")
async def health_check():
    """检查服务状态"""
    return {
        "status": "healthy",
        "vqa_model_loaded": vqa_model is not None,
        "clip_model_loaded": clip_model is not None,
        "image_library_size": len(image_library),
        "device": DEVICE
    }

# ====================================
# 启动服务器
# ====================================
if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 60)
    print("  多模态融合服务器启动")
    print("=" * 60)
    print(f"  服务地址: http://{SERVER_HOST}:{SERVER_PORT}")
    print(f"  图片库路径: {os.path.abspath(IMAGE_LIBRARY_PATH)}")
    print(f"  计算设备: {DEVICE}")
    print("=" * 60 + "\n")
    
    uvicorn.run(
        app,
        host=SERVER_HOST,
        port=SERVER_PORT,
        log_level="info"
    )
